I0216 04:18:17.586401 175608 caffe.cpp:108] Use GPU with device ID 0
I0216 04:18:17.911419 175608 common.cpp:22] System entropy source not available, using fallback algorithm to generate seed instead.
I0216 04:18:17.911419 175608 caffe.cpp:116] Starting Optimization
I0216 04:18:17.911419 175608 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "lenet"
solver_mode: GPU
random_seed: 1234
net: "lenet_BN_train_valid.prototxt"
solver_type: SGD
delta: 1e-008
I0216 04:18:17.911419 175608 solver.cpp:77] Creating training net from net file: lenet_BN_train_valid.prototxt
E0216 04:18:17.911419 175608 upgrade_proto.cpp:603] Attempting to upgrade input file specified using deprecated transformation parameters: lenet_BN_train_valid.prototxt
I0216 04:18:17.911419 175608 upgrade_proto.cpp:606] Successfully upgraded file specified using deprecated data transformation parameters.
E0216 04:18:17.911419 175608 upgrade_proto.cpp:608] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0216 04:18:17.911419 175608 net.cpp:275] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0216 04:18:17.912420 175608 net.cpp:275] The NetState phase (0) differed from the phase (1) specified by a rule in layer prob
I0216 04:18:17.912420 175608 net.cpp:275] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0216 04:18:17.912420 175608 net.cpp:39] Initializing net from parameters: 
name: "LeNet"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "mnist-train-leveldb"
    batch_size: 100
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1_bn"
  name: "conv1_bn"
  type: BN
  blobs_lr: 1
  blobs_lr: 1
  weight_decay: 0
  weight_decay: 0
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1_bn"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2_bn"
  name: "conv2_bn"
  type: BN
  blobs_lr: 1
  blobs_lr: 1
  weight_decay: 0
  weight_decay: 0
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv2_bn"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  bottom: "label"
  name: "loss"
  type: SOFTMAX_LOSS
  include {
    phase: TRAIN
  }
}
state {
  phase: TRAIN
}
I0216 04:18:17.912420 175608 net.cpp:67] Creating Layer mnist
I0216 04:18:17.912420 175608 net.cpp:356] mnist -> data
I0216 04:18:17.912420 175608 net.cpp:356] mnist -> label
I0216 04:18:17.912420 175608 net.cpp:96] Setting up mnist
I0216 04:18:17.912420 175608 data_layer.cpp:46] Opening leveldb mnist-train-leveldb
I0216 04:18:17.920420 175608 data_layer.cpp:130] output data size: 100,1,28,28
I0216 04:18:17.920420 175608 net.cpp:103] Top shape: 100 1 28 28 (78400)
I0216 04:18:17.921421 175608 net.cpp:103] Top shape: 100 1 1 1 (100)
I0216 04:18:17.921421 175608 net.cpp:67] Creating Layer conv1
I0216 04:18:17.921421 175608 net.cpp:394] conv1 <- data
I0216 04:18:17.921421 175608 net.cpp:356] conv1 -> conv1
I0216 04:18:17.921421 175608 net.cpp:96] Setting up conv1
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 20 24 24 (1152000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer conv1_bn
I0216 04:18:17.955422 175608 net.cpp:394] conv1_bn <- conv1
I0216 04:18:17.955422 175608 net.cpp:356] conv1_bn -> conv1_bn
I0216 04:18:17.955422 175608 net.cpp:96] Setting up conv1_bn
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 20 24 24 (1152000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer pool1
I0216 04:18:17.955422 175608 net.cpp:394] pool1 <- conv1_bn
I0216 04:18:17.955422 175608 net.cpp:356] pool1 -> pool1
I0216 04:18:17.955422 175608 net.cpp:96] Setting up pool1
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 20 12 12 (288000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer conv2
I0216 04:18:17.955422 175608 net.cpp:394] conv2 <- pool1
I0216 04:18:17.955422 175608 net.cpp:356] conv2 -> conv2
I0216 04:18:17.955422 175608 net.cpp:96] Setting up conv2
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 50 8 8 (320000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer conv2_bn
I0216 04:18:17.955422 175608 net.cpp:394] conv2_bn <- conv2
I0216 04:18:17.955422 175608 net.cpp:356] conv2_bn -> conv2_bn
I0216 04:18:17.955422 175608 net.cpp:96] Setting up conv2_bn
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 50 8 8 (320000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer pool2
I0216 04:18:17.955422 175608 net.cpp:394] pool2 <- conv2_bn
I0216 04:18:17.955422 175608 net.cpp:356] pool2 -> pool2
I0216 04:18:17.955422 175608 net.cpp:96] Setting up pool2
I0216 04:18:17.955422 175608 net.cpp:103] Top shape: 100 50 4 4 (80000)
I0216 04:18:17.955422 175608 net.cpp:67] Creating Layer ip1
I0216 04:18:17.955422 175608 net.cpp:394] ip1 <- pool2
I0216 04:18:17.955422 175608 net.cpp:356] ip1 -> ip1
I0216 04:18:17.955422 175608 net.cpp:96] Setting up ip1
I0216 04:18:17.959422 175608 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0216 04:18:17.959422 175608 net.cpp:67] Creating Layer relu1
I0216 04:18:17.959422 175608 net.cpp:394] relu1 <- ip1
I0216 04:18:17.959422 175608 net.cpp:345] relu1 -> ip1 (in-place)
I0216 04:18:17.959422 175608 net.cpp:96] Setting up relu1
I0216 04:18:17.959422 175608 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0216 04:18:17.959422 175608 net.cpp:67] Creating Layer ip2
I0216 04:18:17.959422 175608 net.cpp:394] ip2 <- ip1
I0216 04:18:17.959422 175608 net.cpp:356] ip2 -> ip2
I0216 04:18:17.959422 175608 net.cpp:96] Setting up ip2
I0216 04:18:17.960422 175608 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0216 04:18:17.960422 175608 net.cpp:67] Creating Layer loss
I0216 04:18:17.960422 175608 net.cpp:394] loss <- ip2
I0216 04:18:17.960422 175608 net.cpp:394] loss <- label
I0216 04:18:17.960422 175608 net.cpp:356] loss -> (automatic)
I0216 04:18:17.960422 175608 net.cpp:96] Setting up loss
I0216 04:18:17.960422 175608 net.cpp:103] Top shape: 1 1 1 1 (1)
I0216 04:18:17.960422 175608 net.cpp:109]     with loss weight 1
I0216 04:18:17.960422 175608 net.cpp:170] loss needs backward computation.
I0216 04:18:17.960422 175608 net.cpp:170] ip2 needs backward computation.
I0216 04:18:17.960422 175608 net.cpp:170] relu1 needs backward computation.
I0216 04:18:17.960422 175608 net.cpp:170] ip1 needs backward computation.
I0216 04:18:17.960422 175608 net.cpp:170] pool2 needs backward computation.
I0216 04:18:17.960422 175608 net.cpp:170] conv2_bn needs backward computation.
I0216 04:18:18.116431 175608 net.cpp:170] conv2 needs backward computation.
I0216 04:18:18.116431 175608 net.cpp:170] pool1 needs backward computation.
I0216 04:18:18.116431 175608 net.cpp:170] conv1_bn needs backward computation.
I0216 04:18:18.116431 175608 net.cpp:170] conv1 needs backward computation.
I0216 04:18:18.116431 175608 net.cpp:172] mnist does not need backward computation.
I0216 04:18:18.116431 175608 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0216 04:18:18.116431 175608 net.cpp:219] Network initialization done.
I0216 04:18:18.116431 175608 net.cpp:220] Memory required for data: 13966004
E0216 04:18:18.116431 175608 upgrade_proto.cpp:603] Attempting to upgrade input file specified using deprecated transformation parameters: lenet_BN_train_valid.prototxt
I0216 04:18:18.116431 175608 upgrade_proto.cpp:606] Successfully upgraded file specified using deprecated data transformation parameters.
E0216 04:18:18.116431 175608 upgrade_proto.cpp:608] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0216 04:18:18.116431 175608 solver.cpp:161] Creating test net (#0) specified by net file: lenet_BN_train_valid.prototxt
I0216 04:18:18.116431 175608 net.cpp:275] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0216 04:18:18.116431 175608 net.cpp:275] The NetState phase (1) differed from the phase (0) specified by a rule in layer loss
I0216 04:18:18.116431 175608 net.cpp:39] Initializing net from parameters: 
name: "LeNet"
layers {
  top: "data"
  top: "label"
  name: "mnist"
  type: DATA
  data_param {
    source: "mnist-test-leveldb"
    batch_size: 100
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1_bn"
  name: "conv1_bn"
  type: BN
  blobs_lr: 1
  blobs_lr: 1
  weight_decay: 0
  weight_decay: 0
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1_bn"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2_bn"
  name: "conv2_bn"
  type: BN
  blobs_lr: 1
  blobs_lr: 1
  weight_decay: 0
  weight_decay: 0
  bn_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    shift_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv2_bn"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "ip1"
  name: "ip1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip1"
  top: "ip1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "ip1"
  top: "ip2"
  name: "ip2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layers {
  bottom: "ip2"
  top: "prob"
  name: "prob"
  type: SOFTMAX
  include {
    phase: TEST
  }
}
layers {
  bottom: "prob"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
  include {
    phase: TEST
  }
}
state {
  phase: TEST
}
I0216 04:18:18.116431 175608 net.cpp:67] Creating Layer mnist
I0216 04:18:18.216437 175608 net.cpp:356] mnist -> data
I0216 04:18:18.216437 175608 net.cpp:356] mnist -> label
I0216 04:18:18.216437 175608 net.cpp:96] Setting up mnist
I0216 04:18:18.216437 175608 data_layer.cpp:46] Opening leveldb mnist-test-leveldb
I0216 04:18:18.223438 175608 data_layer.cpp:130] output data size: 100,1,28,28
I0216 04:18:18.223438 175608 net.cpp:103] Top shape: 100 1 28 28 (78400)
I0216 04:18:18.223438 175608 net.cpp:103] Top shape: 100 1 1 1 (100)
I0216 04:18:18.223438 175608 net.cpp:67] Creating Layer conv1
I0216 04:18:18.223438 175608 net.cpp:394] conv1 <- data
I0216 04:18:18.223438 175608 net.cpp:356] conv1 -> conv1
I0216 04:18:18.223438 175608 net.cpp:96] Setting up conv1
I0216 04:18:18.223438 175608 net.cpp:103] Top shape: 100 20 24 24 (1152000)
I0216 04:18:18.223438 175608 net.cpp:67] Creating Layer conv1_bn
I0216 04:18:18.223438 175608 net.cpp:394] conv1_bn <- conv1
I0216 04:18:18.223438 175608 net.cpp:356] conv1_bn -> conv1_bn
I0216 04:18:18.223438 175608 net.cpp:96] Setting up conv1_bn
I0216 04:18:18.223438 175608 net.cpp:103] Top shape: 100 20 24 24 (1152000)
I0216 04:18:18.223438 175608 net.cpp:67] Creating Layer pool1
I0216 04:18:18.223438 175608 net.cpp:394] pool1 <- conv1_bn
I0216 04:18:18.223438 175608 net.cpp:356] pool1 -> pool1
I0216 04:18:18.223438 175608 net.cpp:96] Setting up pool1
I0216 04:18:18.223438 175608 net.cpp:103] Top shape: 100 20 12 12 (288000)
I0216 04:18:18.223438 175608 net.cpp:67] Creating Layer conv2
I0216 04:18:18.223438 175608 net.cpp:394] conv2 <- pool1
I0216 04:18:18.223438 175608 net.cpp:356] conv2 -> conv2
I0216 04:18:18.223438 175608 net.cpp:96] Setting up conv2
I0216 04:18:18.224437 175608 net.cpp:103] Top shape: 100 50 8 8 (320000)
I0216 04:18:18.224437 175608 net.cpp:67] Creating Layer conv2_bn
I0216 04:18:18.224437 175608 net.cpp:394] conv2_bn <- conv2
I0216 04:18:18.224437 175608 net.cpp:356] conv2_bn -> conv2_bn
I0216 04:18:18.224437 175608 net.cpp:96] Setting up conv2_bn
I0216 04:18:18.224437 175608 net.cpp:103] Top shape: 100 50 8 8 (320000)
I0216 04:18:18.224437 175608 net.cpp:67] Creating Layer pool2
I0216 04:18:18.224437 175608 net.cpp:394] pool2 <- conv2_bn
I0216 04:18:18.224437 175608 net.cpp:356] pool2 -> pool2
I0216 04:18:18.224437 175608 net.cpp:96] Setting up pool2
I0216 04:18:18.224437 175608 net.cpp:103] Top shape: 100 50 4 4 (80000)
I0216 04:18:18.224437 175608 net.cpp:67] Creating Layer ip1
I0216 04:18:18.224437 175608 net.cpp:394] ip1 <- pool2
I0216 04:18:18.224437 175608 net.cpp:356] ip1 -> ip1
I0216 04:18:18.224437 175608 net.cpp:96] Setting up ip1
I0216 04:18:18.227438 175608 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0216 04:18:18.227438 175608 net.cpp:67] Creating Layer relu1
I0216 04:18:18.227438 175608 net.cpp:394] relu1 <- ip1
I0216 04:18:18.227438 175608 net.cpp:345] relu1 -> ip1 (in-place)
I0216 04:18:18.227438 175608 net.cpp:96] Setting up relu1
I0216 04:18:18.227438 175608 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0216 04:18:18.227438 175608 net.cpp:67] Creating Layer ip2
I0216 04:18:18.227438 175608 net.cpp:394] ip2 <- ip1
I0216 04:18:18.227438 175608 net.cpp:356] ip2 -> ip2
I0216 04:18:18.227438 175608 net.cpp:96] Setting up ip2
I0216 04:18:18.227438 175608 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0216 04:18:18.227438 175608 net.cpp:67] Creating Layer prob
I0216 04:18:18.227438 175608 net.cpp:394] prob <- ip2
I0216 04:18:18.227438 175608 net.cpp:356] prob -> prob
I0216 04:18:18.227438 175608 net.cpp:96] Setting up prob
I0216 04:18:18.227438 175608 net.cpp:103] Top shape: 100 10 1 1 (1000)
I0216 04:18:18.227438 175608 net.cpp:67] Creating Layer accuracy
I0216 04:18:18.227438 175608 net.cpp:394] accuracy <- prob
I0216 04:18:18.227438 175608 net.cpp:394] accuracy <- label
I0216 04:18:18.227438 175608 net.cpp:356] accuracy -> accuracy
I0216 04:18:18.227438 175608 net.cpp:96] Setting up accuracy
I0216 04:18:18.227438 175608 net.cpp:103] Top shape: 1 1 1 1 (1)
I0216 04:18:18.227438 175608 net.cpp:172] accuracy does not need backward computation.
I0216 04:18:18.227438 175608 net.cpp:172] prob does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] ip2 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] relu1 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] ip1 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] pool2 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] conv2_bn does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] conv2 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] pool1 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] conv1_bn does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] conv1 does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:172] mnist does not need backward computation.
I0216 04:18:18.228438 175608 net.cpp:208] This network produces output accuracy
I0216 04:18:18.228438 175608 net.cpp:467] Collecting Learning Rate and Weight Decay.
I0216 04:18:18.228438 175608 net.cpp:219] Network initialization done.
I0216 04:18:18.228438 175608 net.cpp:220] Memory required for data: 13970004
I0216 04:18:18.228438 175608 solver.cpp:51] Solver scaffolding done.
I0216 04:18:18.228438 175608 solver.cpp:170] Solving LeNet
I0216 04:18:18.228438 175608 solver.cpp:287] Iteration 0, Testing net (#0)
I0216 04:18:18.552456 175608 solver.cpp:338]     Test net output #0: accuracy = 0.0916
I0216 04:18:18.567457 175608 solver.cpp:231] Iteration 0, loss = 2.30219
I0216 04:18:18.567457 175608 solver.cpp:442] Iteration 0, lr = 0.01
I0216 04:18:19.416507 175608 solver.cpp:231] Iteration 100, loss = 0.323583
I0216 04:18:19.416507 175608 solver.cpp:442] Iteration 100, lr = 0.00992565
I0216 04:18:20.255554 175608 solver.cpp:231] Iteration 200, loss = 0.238177
I0216 04:18:20.255554 175608 solver.cpp:442] Iteration 200, lr = 0.00985258
I0216 04:18:21.126603 175608 solver.cpp:231] Iteration 300, loss = 0.147898
I0216 04:18:21.126603 175608 solver.cpp:442] Iteration 300, lr = 0.00978075
I0216 04:18:21.964651 175608 solver.cpp:231] Iteration 400, loss = 0.0977781
I0216 04:18:21.964651 175608 solver.cpp:442] Iteration 400, lr = 0.00971013
I0216 04:18:22.852702 175608 solver.cpp:287] Iteration 500, Testing net (#0)
I0216 04:18:23.140719 175608 solver.cpp:338]     Test net output #0: accuracy = 0.975
I0216 04:18:23.144719 175608 solver.cpp:231] Iteration 500, loss = 0.0880777
I0216 04:18:23.144719 175608 solver.cpp:442] Iteration 500, lr = 0.00964069
I0216 04:18:23.952765 175608 solver.cpp:231] Iteration 600, loss = 0.137506
I0216 04:18:23.952765 175608 solver.cpp:442] Iteration 600, lr = 0.0095724
I0216 04:18:24.751811 175608 solver.cpp:231] Iteration 700, loss = 0.0832007
I0216 04:18:24.751811 175608 solver.cpp:442] Iteration 700, lr = 0.00950522
I0216 04:18:25.587859 175608 solver.cpp:231] Iteration 800, loss = 0.117091
I0216 04:18:25.587859 175608 solver.cpp:442] Iteration 800, lr = 0.00943913
I0216 04:18:26.549914 175608 solver.cpp:231] Iteration 900, loss = 0.0944706
I0216 04:18:26.549914 175608 solver.cpp:442] Iteration 900, lr = 0.00937411
I0216 04:18:27.412963 175608 solver.cpp:287] Iteration 1000, Testing net (#0)
I0216 04:18:27.739982 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9809
I0216 04:18:27.743983 175608 solver.cpp:231] Iteration 1000, loss = 0.0442905
I0216 04:18:27.743983 175608 solver.cpp:442] Iteration 1000, lr = 0.00931012
I0216 04:18:28.542028 175608 solver.cpp:231] Iteration 1100, loss = 0.0386893
I0216 04:18:28.542028 175608 solver.cpp:442] Iteration 1100, lr = 0.00924715
I0216 04:18:29.340073 175608 solver.cpp:231] Iteration 1200, loss = 0.090433
I0216 04:18:29.340073 175608 solver.cpp:442] Iteration 1200, lr = 0.00918515
I0216 04:18:30.138119 175608 solver.cpp:231] Iteration 1300, loss = 0.0511724
I0216 04:18:30.138119 175608 solver.cpp:442] Iteration 1300, lr = 0.00912412
I0216 04:18:30.939165 175608 solver.cpp:231] Iteration 1400, loss = 0.0780624
I0216 04:18:30.939165 175608 solver.cpp:442] Iteration 1400, lr = 0.00906403
I0216 04:18:31.735210 175608 solver.cpp:287] Iteration 1500, Testing net (#0)
I0216 04:18:32.023227 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9848
I0216 04:18:32.027227 175608 solver.cpp:231] Iteration 1500, loss = 0.0651043
I0216 04:18:32.027227 175608 solver.cpp:442] Iteration 1500, lr = 0.00900485
I0216 04:18:32.825273 175608 solver.cpp:231] Iteration 1600, loss = 0.0313462
I0216 04:18:32.825273 175608 solver.cpp:442] Iteration 1600, lr = 0.00894657
I0216 04:18:33.628319 175608 solver.cpp:231] Iteration 1700, loss = 0.0319933
I0216 04:18:33.628319 175608 solver.cpp:442] Iteration 1700, lr = 0.00888916
I0216 04:18:34.437366 175608 solver.cpp:231] Iteration 1800, loss = 0.071528
I0216 04:18:34.437366 175608 solver.cpp:442] Iteration 1800, lr = 0.0088326
I0216 04:18:35.237411 175608 solver.cpp:231] Iteration 1900, loss = 0.0289448
I0216 04:18:35.237411 175608 solver.cpp:442] Iteration 1900, lr = 0.00877687
I0216 04:18:36.035456 175608 solver.cpp:287] Iteration 2000, Testing net (#0)
I0216 04:18:36.320472 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9869
I0216 04:18:36.324473 175608 solver.cpp:231] Iteration 2000, loss = 0.0652831
I0216 04:18:36.324473 175608 solver.cpp:442] Iteration 2000, lr = 0.00872196
I0216 04:18:37.118518 175608 solver.cpp:231] Iteration 2100, loss = 0.0495987
I0216 04:18:37.118518 175608 solver.cpp:442] Iteration 2100, lr = 0.00866784
I0216 04:18:37.914564 175608 solver.cpp:231] Iteration 2200, loss = 0.0240206
I0216 04:18:37.914564 175608 solver.cpp:442] Iteration 2200, lr = 0.0086145
I0216 04:18:38.709609 175608 solver.cpp:231] Iteration 2300, loss = 0.0274047
I0216 04:18:38.709609 175608 solver.cpp:442] Iteration 2300, lr = 0.00856192
I0216 04:18:39.501654 175608 solver.cpp:231] Iteration 2400, loss = 0.0494625
I0216 04:18:39.501654 175608 solver.cpp:442] Iteration 2400, lr = 0.00851008
I0216 04:18:40.292701 175608 solver.cpp:287] Iteration 2500, Testing net (#0)
I0216 04:18:40.577716 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9859
I0216 04:18:40.581717 175608 solver.cpp:231] Iteration 2500, loss = 0.019131
I0216 04:18:40.581717 175608 solver.cpp:442] Iteration 2500, lr = 0.00845897
I0216 04:18:41.377763 175608 solver.cpp:231] Iteration 2600, loss = 0.0498539
I0216 04:18:41.377763 175608 solver.cpp:442] Iteration 2600, lr = 0.00840857
I0216 04:18:42.173807 175608 solver.cpp:231] Iteration 2700, loss = 0.041836
I0216 04:18:42.173807 175608 solver.cpp:442] Iteration 2700, lr = 0.00835886
I0216 04:18:42.970854 175608 solver.cpp:231] Iteration 2800, loss = 0.0180708
I0216 04:18:42.970854 175608 solver.cpp:442] Iteration 2800, lr = 0.00830984
I0216 04:18:43.765898 175608 solver.cpp:231] Iteration 2900, loss = 0.0177104
I0216 04:18:43.765898 175608 solver.cpp:442] Iteration 2900, lr = 0.00826148
I0216 04:18:44.552943 175608 solver.cpp:287] Iteration 3000, Testing net (#0)
I0216 04:18:44.838960 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9863
I0216 04:18:44.842960 175608 solver.cpp:231] Iteration 3000, loss = 0.0276337
I0216 04:18:44.842960 175608 solver.cpp:442] Iteration 3000, lr = 0.00821377
I0216 04:18:45.638005 175608 solver.cpp:231] Iteration 3100, loss = 0.0144901
I0216 04:18:45.638005 175608 solver.cpp:442] Iteration 3100, lr = 0.0081667
I0216 04:18:46.439051 175608 solver.cpp:231] Iteration 3200, loss = 0.03431
I0216 04:18:46.439051 175608 solver.cpp:442] Iteration 3200, lr = 0.00812025
I0216 04:18:47.237097 175608 solver.cpp:231] Iteration 3300, loss = 0.032456
I0216 04:18:47.237097 175608 solver.cpp:442] Iteration 3300, lr = 0.00807442
I0216 04:18:48.028142 175608 solver.cpp:231] Iteration 3400, loss = 0.0162868
I0216 04:18:48.028142 175608 solver.cpp:442] Iteration 3400, lr = 0.00802918
I0216 04:18:48.817188 175608 solver.cpp:287] Iteration 3500, Testing net (#0)
I0216 04:18:49.103204 175608 solver.cpp:338]     Test net output #0: accuracy = 0.989
I0216 04:18:49.107204 175608 solver.cpp:231] Iteration 3500, loss = 0.00994866
I0216 04:18:49.107204 175608 solver.cpp:442] Iteration 3500, lr = 0.00798454
I0216 04:18:49.900249 175608 solver.cpp:231] Iteration 3600, loss = 0.0142535
I0216 04:18:49.900249 175608 solver.cpp:442] Iteration 3600, lr = 0.00794046
I0216 04:18:50.697295 175608 solver.cpp:231] Iteration 3700, loss = 0.0129755
I0216 04:18:50.697295 175608 solver.cpp:442] Iteration 3700, lr = 0.00789695
I0216 04:18:51.493340 175608 solver.cpp:231] Iteration 3800, loss = 0.0213048
I0216 04:18:51.493340 175608 solver.cpp:442] Iteration 3800, lr = 0.007854
I0216 04:18:52.289386 175608 solver.cpp:231] Iteration 3900, loss = 0.0250812
I0216 04:18:52.289386 175608 solver.cpp:442] Iteration 3900, lr = 0.00781158
I0216 04:18:53.071431 175608 solver.cpp:287] Iteration 4000, Testing net (#0)
I0216 04:18:53.357447 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9891
I0216 04:18:53.361448 175608 solver.cpp:231] Iteration 4000, loss = 0.0133379
I0216 04:18:53.361448 175608 solver.cpp:442] Iteration 4000, lr = 0.00776969
I0216 04:18:54.155493 175608 solver.cpp:231] Iteration 4100, loss = 0.00584425
I0216 04:18:54.155493 175608 solver.cpp:442] Iteration 4100, lr = 0.00772833
I0216 04:18:54.949538 175608 solver.cpp:231] Iteration 4200, loss = 0.00811632
I0216 04:18:54.949538 175608 solver.cpp:442] Iteration 4200, lr = 0.00768748
I0216 04:18:55.751585 175608 solver.cpp:231] Iteration 4300, loss = 0.0114815
I0216 04:18:55.751585 175608 solver.cpp:442] Iteration 4300, lr = 0.00764712
I0216 04:18:56.551630 175608 solver.cpp:231] Iteration 4400, loss = 0.0145507
I0216 04:18:56.551630 175608 solver.cpp:442] Iteration 4400, lr = 0.00760726
I0216 04:18:57.344676 175608 solver.cpp:287] Iteration 4500, Testing net (#0)
I0216 04:18:57.630692 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9896
I0216 04:18:57.634692 175608 solver.cpp:231] Iteration 4500, loss = 0.0194373
I0216 04:18:57.634692 175608 solver.cpp:442] Iteration 4500, lr = 0.00756788
I0216 04:18:58.435737 175608 solver.cpp:231] Iteration 4600, loss = 0.0109503
I0216 04:18:58.435737 175608 solver.cpp:442] Iteration 4600, lr = 0.00752897
I0216 04:18:59.234783 175608 solver.cpp:231] Iteration 4700, loss = 0.00481297
I0216 04:18:59.234783 175608 solver.cpp:442] Iteration 4700, lr = 0.00749052
I0216 04:19:00.032829 175608 solver.cpp:231] Iteration 4800, loss = 0.00579256
I0216 04:19:00.032829 175608 solver.cpp:442] Iteration 4800, lr = 0.00745253
I0216 04:19:00.831874 175608 solver.cpp:231] Iteration 4900, loss = 0.0112305
I0216 04:19:00.831874 175608 solver.cpp:442] Iteration 4900, lr = 0.00741498
I0216 04:19:01.628921 175608 solver.cpp:356] Snapshotting to lenet_iter_5000.caffemodel
I0216 04:19:01.635921 175608 solver.cpp:363] Snapshotting solver state to lenet_iter_5000.caffemodel.solverstate
I0216 04:19:01.639921 175608 solver.cpp:287] Iteration 5000, Testing net (#0)
I0216 04:19:01.926937 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9902
I0216 04:19:01.930938 175608 solver.cpp:231] Iteration 5000, loss = 0.0107038
I0216 04:19:01.930938 175608 solver.cpp:442] Iteration 5000, lr = 0.00737788
I0216 04:19:02.731983 175608 solver.cpp:231] Iteration 5100, loss = 0.0143447
I0216 04:19:02.731983 175608 solver.cpp:442] Iteration 5100, lr = 0.0073412
I0216 04:19:03.530030 175608 solver.cpp:231] Iteration 5200, loss = 0.0091249
I0216 04:19:03.530030 175608 solver.cpp:442] Iteration 5200, lr = 0.00730495
I0216 04:19:04.330075 175608 solver.cpp:231] Iteration 5300, loss = 0.00372573
I0216 04:19:04.330075 175608 solver.cpp:442] Iteration 5300, lr = 0.00726911
I0216 04:19:05.127120 175608 solver.cpp:231] Iteration 5400, loss = 0.00464001
I0216 04:19:05.127120 175608 solver.cpp:442] Iteration 5400, lr = 0.00723368
I0216 04:19:05.919165 175608 solver.cpp:287] Iteration 5500, Testing net (#0)
I0216 04:19:06.209182 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9882
I0216 04:19:06.213182 175608 solver.cpp:231] Iteration 5500, loss = 0.0120706
I0216 04:19:06.213182 175608 solver.cpp:442] Iteration 5500, lr = 0.00719865
I0216 04:19:07.020228 175608 solver.cpp:231] Iteration 5600, loss = 0.00932962
I0216 04:19:07.020228 175608 solver.cpp:442] Iteration 5600, lr = 0.00716402
I0216 04:19:07.819274 175608 solver.cpp:231] Iteration 5700, loss = 0.0112366
I0216 04:19:07.820274 175608 solver.cpp:442] Iteration 5700, lr = 0.00712977
I0216 04:19:08.618320 175608 solver.cpp:231] Iteration 5800, loss = 0.00613519
I0216 04:19:08.618320 175608 solver.cpp:442] Iteration 5800, lr = 0.0070959
I0216 04:19:09.420367 175608 solver.cpp:231] Iteration 5900, loss = 0.00257038
I0216 04:19:09.420367 175608 solver.cpp:442] Iteration 5900, lr = 0.0070624
I0216 04:19:10.216411 175608 solver.cpp:287] Iteration 6000, Testing net (#0)
I0216 04:19:10.507428 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9889
I0216 04:19:10.510428 175608 solver.cpp:231] Iteration 6000, loss = 0.00416729
I0216 04:19:10.510428 175608 solver.cpp:442] Iteration 6000, lr = 0.00702927
I0216 04:19:11.310474 175608 solver.cpp:231] Iteration 6100, loss = 0.0109228
I0216 04:19:11.310474 175608 solver.cpp:442] Iteration 6100, lr = 0.0069965
I0216 04:19:12.112520 175608 solver.cpp:231] Iteration 6200, loss = 0.00840149
I0216 04:19:12.112520 175608 solver.cpp:442] Iteration 6200, lr = 0.00696408
I0216 04:19:12.915566 175608 solver.cpp:231] Iteration 6300, loss = 0.00884235
I0216 04:19:12.915566 175608 solver.cpp:442] Iteration 6300, lr = 0.00693201
I0216 04:19:13.720613 175608 solver.cpp:231] Iteration 6400, loss = 0.00446463
I0216 04:19:13.720613 175608 solver.cpp:442] Iteration 6400, lr = 0.00690029
I0216 04:19:14.517657 175608 solver.cpp:287] Iteration 6500, Testing net (#0)
I0216 04:19:14.808675 175608 solver.cpp:338]     Test net output #0: accuracy = 0.99
I0216 04:19:14.812674 175608 solver.cpp:231] Iteration 6500, loss = 0.0021917
I0216 04:19:14.812674 175608 solver.cpp:442] Iteration 6500, lr = 0.0068689
I0216 04:19:15.617720 175608 solver.cpp:231] Iteration 6600, loss = 0.00433947
I0216 04:19:15.617720 175608 solver.cpp:442] Iteration 6600, lr = 0.00683784
I0216 04:19:16.424767 175608 solver.cpp:231] Iteration 6700, loss = 0.00859663
I0216 04:19:16.424767 175608 solver.cpp:442] Iteration 6700, lr = 0.00680711
I0216 04:19:17.230813 175608 solver.cpp:231] Iteration 6800, loss = 0.00789779
I0216 04:19:17.230813 175608 solver.cpp:442] Iteration 6800, lr = 0.0067767
I0216 04:19:18.038859 175608 solver.cpp:231] Iteration 6900, loss = 0.00722114
I0216 04:19:18.038859 175608 solver.cpp:442] Iteration 6900, lr = 0.0067466
I0216 04:19:18.838904 175608 solver.cpp:287] Iteration 7000, Testing net (#0)
I0216 04:19:19.126921 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9901
I0216 04:19:19.130921 175608 solver.cpp:231] Iteration 7000, loss = 0.00331413
I0216 04:19:19.130921 175608 solver.cpp:442] Iteration 7000, lr = 0.00671681
I0216 04:19:19.934967 175608 solver.cpp:231] Iteration 7100, loss = 0.00194773
I0216 04:19:19.934967 175608 solver.cpp:442] Iteration 7100, lr = 0.00668733
I0216 04:19:20.738013 175608 solver.cpp:231] Iteration 7200, loss = 0.00372049
I0216 04:19:20.738013 175608 solver.cpp:442] Iteration 7200, lr = 0.00665815
I0216 04:19:21.546059 175608 solver.cpp:231] Iteration 7300, loss = 0.00741673
I0216 04:19:21.546059 175608 solver.cpp:442] Iteration 7300, lr = 0.00662927
I0216 04:19:22.352105 175608 solver.cpp:231] Iteration 7400, loss = 0.00660062
I0216 04:19:22.352105 175608 solver.cpp:442] Iteration 7400, lr = 0.00660067
I0216 04:19:23.148151 175608 solver.cpp:287] Iteration 7500, Testing net (#0)
I0216 04:19:23.438168 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9898
I0216 04:19:23.442168 175608 solver.cpp:231] Iteration 7500, loss = 0.00613836
I0216 04:19:23.442168 175608 solver.cpp:442] Iteration 7500, lr = 0.00657236
I0216 04:19:24.245214 175608 solver.cpp:231] Iteration 7600, loss = 0.00259174
I0216 04:19:24.245214 175608 solver.cpp:442] Iteration 7600, lr = 0.00654433
I0216 04:19:25.049260 175608 solver.cpp:231] Iteration 7700, loss = 0.00195477
I0216 04:19:25.049260 175608 solver.cpp:442] Iteration 7700, lr = 0.00651658
I0216 04:19:25.853307 175608 solver.cpp:231] Iteration 7800, loss = 0.00377298
I0216 04:19:25.853307 175608 solver.cpp:442] Iteration 7800, lr = 0.00648911
I0216 04:19:26.659353 175608 solver.cpp:231] Iteration 7900, loss = 0.00625868
I0216 04:19:26.659353 175608 solver.cpp:442] Iteration 7900, lr = 0.0064619
I0216 04:19:27.457397 175608 solver.cpp:287] Iteration 8000, Testing net (#0)
I0216 04:19:27.746414 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9907
I0216 04:19:27.750414 175608 solver.cpp:231] Iteration 8000, loss = 0.00585751
I0216 04:19:27.750414 175608 solver.cpp:442] Iteration 8000, lr = 0.00643496
I0216 04:19:28.552460 175608 solver.cpp:231] Iteration 8100, loss = 0.00529869
I0216 04:19:28.552460 175608 solver.cpp:442] Iteration 8100, lr = 0.00640827
I0216 04:19:29.356506 175608 solver.cpp:231] Iteration 8200, loss = 0.00240924
I0216 04:19:29.356506 175608 solver.cpp:442] Iteration 8200, lr = 0.00638185
I0216 04:19:30.157552 175608 solver.cpp:231] Iteration 8300, loss = 0.00209122
I0216 04:19:30.157552 175608 solver.cpp:442] Iteration 8300, lr = 0.00635568
I0216 04:19:30.963598 175608 solver.cpp:231] Iteration 8400, loss = 0.00421157
I0216 04:19:30.963598 175608 solver.cpp:442] Iteration 8400, lr = 0.00632975
I0216 04:19:31.760643 175608 solver.cpp:287] Iteration 8500, Testing net (#0)
I0216 04:19:32.049660 175608 solver.cpp:338]     Test net output #0: accuracy = 0.99
I0216 04:19:32.053660 175608 solver.cpp:231] Iteration 8500, loss = 0.00561496
I0216 04:19:32.053660 175608 solver.cpp:442] Iteration 8500, lr = 0.00630407
I0216 04:19:32.860707 175608 solver.cpp:231] Iteration 8600, loss = 0.00516763
I0216 04:19:32.860707 175608 solver.cpp:442] Iteration 8600, lr = 0.00627864
I0216 04:19:33.666753 175608 solver.cpp:231] Iteration 8700, loss = 0.00483749
I0216 04:19:33.666753 175608 solver.cpp:442] Iteration 8700, lr = 0.00625344
I0216 04:19:34.468798 175608 solver.cpp:231] Iteration 8800, loss = 0.00221819
I0216 04:19:34.468798 175608 solver.cpp:442] Iteration 8800, lr = 0.00622847
I0216 04:19:35.273844 175608 solver.cpp:231] Iteration 8900, loss = 0.0022644
I0216 04:19:35.273844 175608 solver.cpp:442] Iteration 8900, lr = 0.00620374
I0216 04:19:36.068891 175608 solver.cpp:287] Iteration 9000, Testing net (#0)
I0216 04:19:36.359906 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9892
I0216 04:19:36.362907 175608 solver.cpp:231] Iteration 9000, loss = 0.00420859
I0216 04:19:36.362907 175608 solver.cpp:442] Iteration 9000, lr = 0.00617924
I0216 04:19:37.168953 175608 solver.cpp:231] Iteration 9100, loss = 0.00536309
I0216 04:19:37.168953 175608 solver.cpp:442] Iteration 9100, lr = 0.00615496
I0216 04:19:37.973999 175608 solver.cpp:231] Iteration 9200, loss = 0.00526801
I0216 04:19:37.973999 175608 solver.cpp:442] Iteration 9200, lr = 0.0061309
I0216 04:19:38.777045 175608 solver.cpp:231] Iteration 9300, loss = 0.00417334
I0216 04:19:38.777045 175608 solver.cpp:442] Iteration 9300, lr = 0.00610706
I0216 04:19:39.579092 175608 solver.cpp:231] Iteration 9400, loss = 0.00202325
I0216 04:19:39.579092 175608 solver.cpp:442] Iteration 9400, lr = 0.00608343
I0216 04:19:40.372136 175608 solver.cpp:287] Iteration 9500, Testing net (#0)
I0216 04:19:40.663153 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9895
I0216 04:19:40.667153 175608 solver.cpp:231] Iteration 9500, loss = 0.00246124
I0216 04:19:40.667153 175608 solver.cpp:442] Iteration 9500, lr = 0.00606002
I0216 04:19:41.471199 175608 solver.cpp:231] Iteration 9600, loss = 0.00385049
I0216 04:19:41.471199 175608 solver.cpp:442] Iteration 9600, lr = 0.00603682
I0216 04:19:42.275245 175608 solver.cpp:231] Iteration 9700, loss = 0.00492551
I0216 04:19:42.275245 175608 solver.cpp:442] Iteration 9700, lr = 0.00601382
I0216 04:19:43.079291 175608 solver.cpp:231] Iteration 9800, loss = 0.00447975
I0216 04:19:43.079291 175608 solver.cpp:442] Iteration 9800, lr = 0.00599102
I0216 04:19:43.889338 175608 solver.cpp:231] Iteration 9900, loss = 0.00405645
I0216 04:19:43.889338 175608 solver.cpp:442] Iteration 9900, lr = 0.00596843
I0216 04:19:44.707384 175608 solver.cpp:356] Snapshotting to lenet_iter_10000.caffemodel
I0216 04:19:44.715384 175608 solver.cpp:363] Snapshotting solver state to lenet_iter_10000.caffemodel.solverstate
I0216 04:19:44.722385 175608 solver.cpp:268] Iteration 10000, loss = 0.00216164
I0216 04:19:44.722385 175608 solver.cpp:287] Iteration 10000, Testing net (#0)
I0216 04:19:45.011401 175608 solver.cpp:338]     Test net output #0: accuracy = 0.9899
I0216 04:19:45.011401 175608 solver.cpp:273] Optimization Done.
I0216 04:19:45.011401 175608 caffe.cpp:130] Optimization Done.
